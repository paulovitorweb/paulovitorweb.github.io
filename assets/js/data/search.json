[ { "title": "7 conselhos para o desenvolvimento de soluções com MongoDB", "url": "/posts/7-conselhos-para-o-desenvolvimento-de-solucoes-com-MongoDB/", "categories": "Banco de dados, MongoDB", "tags": "banco de dados, nosql, database, mongodb, mongo", "date": "2022-09-04 00:00:00 -0300", "snippet": "Depois de passar alguns anos usando apenas bancos de dados relacionais e há mais de um ano trabalhando com MongoDB, passei de alguém que torcia o nariz quando o assunto era bancos de dados não relacionais para alguém que entende o seu propósito e como bancos de dados orientados a documento como o MongoDB podem agregar valor tanto para o desenvolvimento quanto para o negócio.Ao longo desse tempo fui tomando nota de alguns insights e resolvi reunir alguns deles nesse texto, em que elenco 7 conselhos que daria para alguém que está iniciando algum projeto com MongoDB ou mesmo quem já trabalha com ele há algum tempo. Eles podem ajudar a tomar boas decisões desde a modelagem até a construção de consultas.Espero que seja útil.E, claro: críticas e sugestões são bem-vindas.São eles: Use documentos e matrizes incorporados. O MongoDB por muito tempo não ofereceu suporte a transações justamente por não ser uma preocupação primária, afinal a possibilidade de construir um modelo de dados aproveitando incorporações tornava o suporte a transações desnecessário em muitos casos práticos, já que a alteração de um único documento é atômica, e é possível traduzir o modelo normalizado de bancos relacionais em um documento único no MongoDB. Por exemplo, se você tem uma coleção de usuários que podem ter até três endereços cadastrados, considere incorporar os documentos de endereço em um array dentro do documento de usuário. Isso vai evitar a necessidade de buscar essa informações em outra coleção e, em algumas situações, tornar transações desnecessárias, pois a alteração de endereço e de outros dados do usuário será, por definição, atômica; Também é possível explorar o ganho semântico de usar documentos incorporados. Por exemplo, a partir do mesmo exemplo, se cada usuário puder ter apenas um endereço, você pode ainda assim incorporar os dados em um documento dentro do campo de endereço em vez de espalhá-los na raiz do documento, a menos que você tenha um motivo para isso. É mais semântico acessar os dados com endereco.cidade do que usar nomes de campos como endereco_cidade ou enderecoCidade. Além disso, se por algum motivo não for desejável trazer os dados de endereço, apenas não projetar o campo endereco vai desconsiderar tudo que há dentro dele. Ainda, se a aplicação evoluir de forma que deixe de ser interessante armazenar o endereço incorporado na coleção de usuário, vai ser mais fácil fazer a migração para uma coleção separada ou até outro banco (relacional ou não), pois os nomes dos campos poderão permanecer iguais. Semântica importa. Outro exemplo: suponhamos que você tenha uma tabela de posts em que cada post pode ter uma ou mais tags. Neste caso, faz total sentido armazenar as tags em um array dentro do documento de cada post. Ex: tags: [\"bancodedados\", \"mongodb\"]. Além disso, é importante considerar que normalmente o custo de desempenho de se gravar em um único documento é menor do que a transação de vários documentos. De qualquer modo, sempre é necessário refletir sobre a melhor escolha para cada caso específico. Nem sempre incorporar é a melhor opção. O que nos leva ao próximo conselho. O fato de que o MongoDB suporta incorporações e que você pode aplicar isso em diversos níveis de um documento não quer dizer que você deve resumir todo seu banco de dados em uma única coleção. Em outras palavras, não leve a expressão “não relacional” tão ao pé da letra. Relações simples entre documentos em coleções separadas com frequência podem ser o melhor design. Por exemplo, não faz sentido incorporar todas as postagens de um usuário de uma rede social dentro do documento de usuário. Além de essa matriz incorporada poder crescer para além do razoável, é possível pensar em uma série de situações em que você precisará alterar ou buscar as postagens mas não o usuário. Ou ainda precisar de índices em campos dos documentos de posts. De todo modo, com o id indexado na coleção de usuários e armazenado em cada documento na coleção de postagens, você pode buscar a informação de que precisa através de um $lookup com a rapidez e performance garantida pelo índice. Não se assuste se vir bancos de dados MongoDB com dez, vinte, trinta coleções. Ou até mais. “Ah, mas isso não seria transformar o Mongo em um banco relacional?” Bem, você quem pode dizer. No Mongo é possível usar relações. Mas e em um MySQL, por exemplo, você pode usar a estrutura desnormalizada do Mongo? Use a ferramenta certa para o problema certo sem se apegar muito a que ferramenta é “dona” desse ou daquele paradigma de armazenamento de dados. Sempre vão aparecer novas soluções que unem o melhor de dois conceitos diferentes - e, ainda assim, perdem em algum quesito. A forma como o MongoDB usa objetos BSON para executar consultas fornece uma excelente camada de segurança contra os ataques tradicionais de injeção de SQL - você pode encontrar uma explicação sucinta para isso aqui. Mas existe a possibilidade de usar expressões que executam operações JavaScript arbitrárias diretamente no servidor que podem resultar em vulnerabilidade se não forem utilizadas adequadamente. São elas: $where, mapReduce, $accumulator, $function. O ideal é evitar usar essas operações, até porque em muitos casos práticos você pode expressar todas as necessidades de consulta de uma aplicação sem usar JavaScript. De todo modo, você também pode desabilitar todas as execuções de JavaScript no lado do servidor com uma configuração simples descrita aqui. É uma boa prática desabilitar por padrão tudo aquilo que você não precisa e que pode resultar em algum problema de segurança. Pode ser tentador usar o estágio project em uma pipeline para diminuir o tráfego de dados e economizar na largura de banda de rede, mas é preciso ter em mente que isso adicionará mais processamento à consulta e quase sempre resultará num tempo de resposta maior. Reflita sempre sobre os prós e contras de seu uso e sobre o que trará mais valor a um caso específico. Um princípio razoável - que, obviamente, não deve ser levado em consideração isoladamente - é usar project para tasks assíncronas quando for desejável e evitar em consultas síncronas. Há uma exceção para o descrito acima. Uma boa combinação entre índices e projeção pode tornar uma busca muito mais rápida através de uma “consulta coberta”. Por exemplo, imagine que você precise apenas dos ids dos documentos cujo campo tipo seja igual a um determinado valor. Se você adicionar um índice ao campo tipo e, na consulta, usar esse campo como filtro e adicionar um estágio project que traga apenas o id de cada documento - que já é indexado por padrão -, o mongo fará uma consulta coberta, o que significa que ele não precisará buscar os documentos, pois tudo que ele precisa (para buscar e retornar) está nos índices. Este artigo mostrou que uma consulta coberta pode ser até 250 vezes mais rápida do que uma sem índices. As consultas que você precisará fazer na sua aplicação determinam que modelo de dados é o ideal. É importante que você entenda com que frequência e quais dados a sua aplicação mais precisará inserir/buscar/alterar para que possa, então, desenhar o melhor modelo. Boa parte da performance de uma aplicação na camada do banco de dados é resultado de uma boa modelagem, com os índices certos nos lugares certos, e com incorporações sempre que fizerem sentido, e evitando incorporações onde não trouxerem ganho. Por fim: o MongoDB não é bala de prata. Esse é um cliché bem conhecido no mundo da tecnologia, mas é sempre importante lembrar. Primeiro de tudo: dada a possibilidade de inserir documentos em coleções sem obedecer a um schema (embora você possa contornar isso tanto no nível do cliente quanto no nível do próprio mongo), é relativamente fácil as coisas saírem do controle se o time não tiver experiência o suficiente para lidar com essa responsabilidade. Sem o devido cuidado, você pode começar a ter problemas ao ler documentos como, esperando inteiros, receber texto; ou, esperando sempre um documento, receber um valor nulo. O tio Ben já dizia: com grandes poderes vêm grandes responsabilidades. Use clientes com schemas para validar as operações de E/S; O MongoDB tem um excelente desempenho para escrita, mas costuma perder na hora de ler uma grande quantidade de registros em comparação com bancos relacionais. Mesmo que você possa alcançar uma excelente performance na leitura através de índices e de consultas cobertas, se sua aplicação precisa com frequência ler mil, dez mil, cem mil registros, e esses registros podem ficar contidos em uma ou no máximo duas tabelas relacionadas com índices, talvez você deva considerar armazenar esses dados em um banco relacional. Para essa necessidade um MySQL ou PostgreSQL pode ser a melhor escolha, tudo vai depender de cada caso. Se você puder criar um microsserviço para essa necessidade específica que use um banco relacional e deixar o mongo armazenar outros dados que precisem de mais escrita, pode acabar sendo o melhor dos mundos; Bancos relacionais foram feitos para ter um ótimo desempenho quando se precisa de relacionamentos entre tabelas. Os índices e a integridade referencial entre elas garantem a consistência e uma boa performance, mesmo que você precise de duas, três ou mais tabelas para buscar dados que na ponta do processo vão ser um único objeto. Ao elaborar o modelo de dados de uma solução, você pode se deparar com a constatação de que ele é inerentemente relacional. Está lá: você vê que precisa dos dados relacionados tanto juntos quanto, frequentemente, separados. Você precisa de consultas otimizadas por índices em muitos níveis, ou seja, em grande parte das tabelas. Você percebe que a leitura de uma quantidade massiva de dados importa tanto quanto a escrita. É possível ver com clareza os esquemas das tabelas, e a possibilidade de precisar escrever muitas migrações ao longo da via útil da aplicação para ajustar os esquemas rígidos é remota. Todas esses são indícios de que você pode estar diante de um caso em que o melhor caminho é seguir com um banco relacional. " }, { "title": "Importando GeoJSON via url no QGIS", "url": "/posts/Importando-GeoJSON-via-url-no-QGIS/", "categories": "SIG, QGIS", "tags": "sig, qgis, geojson, camada vetorial, geotecnologias", "date": "2022-03-24 00:00:00 -0300", "snippet": "O GeoJSON é um formato de intercâmbio de dados geoespaciais baseado no formato JSON – JavaScript Object Notation. É um recurso muito interessante para quem trabalha com dados espaciais, principalmente pelo vasto uso do JSON para comunicação entre sistemas. É possível, por exemplo, disponibilizar recursos geográficos simples numa URL com atualização regular para aplicações e usuários que precisam do dado atualizado.No QGIS, é possível importar um arquivo GeoJSON disponível em URL na internet. Tão simples quanto importar um shapefile local. Ir em Adicionar Camada Vetorial; Na guia Vetor, no lugar da opção Arquivo, marcar Protocolo: HTTP(s)…; Em Protocolo, escolher Tipo GeoJSON; Em URI, inserir a URL do geojson.Como na imagem abaixo, em que importo os bairros de João Pessoa no QGIS3.Adicionando camada vetorial a partir de um GeoJSON hospedado na internetÉ isso.Camada de bairros de João Pessoa adicionada no QGIS3Se você quer conhecer mais sobre o formato GeoJSON, o RFC 7946  é o documento técnico de referência.Até a próxima." }, { "title": "E se for preciso fazer um lookup com ‘from’ condicional no MongoDB?", "url": "/posts/E-se-for-preciso-fazer-um-lookup-com-from-condicional-no-MongoDB/", "categories": "Banco de dados, MongoDB", "tags": "banco de dados, nosql, database, mongodb, mongo, lookup", "date": "2022-03-19 00:00:00 -0300", "snippet": "Essa semana precisei construir uma pipeline de agregação no MongoDB com um objetivo pouco usual: precisava de um estágio de lookup onde o parâmetro from fosse condicional, do tipo: se o valor do campo X for A, pesquise em uma dada collection, se for B, pesquise em outra.Pense num cenário onde um dado campo do documento armazena um ObjectId que pode ser uma referência a documentos de duas ou mais collections, como uma chave de entidade. Se for preciso buscar o documento dela, como fazer isso numa mesma pipeline e sem perder performance?Uma relação one-to-many em que um campo da entidade poderia conter relação com uma ou outra entidadePensando em um banco relacional, provavelmente a melhor opção seria fazer duas seleções separadas, em que cada uma filtraria pelo valor do campo X correspondente e faria o join com a tabela correta, e depois uni-las com UNION. Mas, como seria o modo NoSQL de conseguir esse tipo de informação?Bem, o desafio foi esse.No começo pensei em coisas como fazer um from condicional:'from': { '$cond': { 'if': { '$eq': ['$field', 'A'] }, 'then': '&lt;uma collection&gt;', 'else': '&lt;outra collection&gt;' }}Claro que deu errado, o parse do mongo avaliou como objeto em vez de avaliar a expressão e acusou um erro de tipo, pois esperava receber uma string. E faz sentido.Então voltei a pensar em como resolveria isso usando SQL - dois selects e uma união - e, trazendo o conceito para o paradigma não relacional, cheguei à pipeline abaixo. É um exemplo fictício. Primeiro, algumas considerações: Estou usando a versão 4.4 do MongoDB, que ainda não suporta a sintaxe concisa para subconsultas no lookup; No exemplo, o campo field_with_id contém o ObjectId que preciso buscar em duas collections diferentes - foo e bar; O campo category contém um dado categórico que me ajuda a descobrir em qual collection vou encontrar o id em field_with_id; Os campos name e data são aqueles que eu quero no objeto buscado.[ { '$lookup': { 'from': 'foo', 'let': { 'idToLookup': '$field_with_id', }, 'pipeline': [ { '$match': { '$expr': { '$eq': ['$_id', '$$idToLookup'] } } }, { '$project': { 'name': 1, 'data': 1 } } ], 'as': 'foo_obj' } }, { '$lookup': { 'from': 'bar', 'let': { 'idToLookup': '$field_with_id', }, 'pipeline': [ { '$match': { '$expr': { '$eq': ['$_id', '$$idToLookup'] } } }, { '$project': { 'name': 1, 'data': 1 } } ], 'as': 'bar_obj' } }, { '$addFields': { 'foo_obj': { '$arrayElemAt': ['$foo_obj', 0] }, 'bar_obj': { '$arrayElemAt': ['$bar_obj', 0] } } }, { '$project': { 'fielda': 1, 'fieldb': 1, 'fieldc': 1, 'searched_obj': { '$switch': { 'branches': [ { 'case': { '$eq': [ '$category', 'A' ] }, 'then': '$foo_obj' }, { 'case': { '$eq': [ '$category', 'B' ] }, 'then': '$bar_obj' } ], 'default': null } } } }]Parece grande, mas na prática são apenas 3 estágios essenciais. A ideia foi executar dois lookups, um para cada collection, e depois usar o switch pra escolher qual objeto seria considerado de acordo com o campo category. O estágio addFields aqui serve apenas para pegar do array o primeiro - e neste caso único - objeto retornado pela pesquisa. Estou usando a sintaxe com subconsulta no lookup para já trazer apenas os campos de interesse name e data e evitar desperdício de memória (um benefício talvez quase nulo a depender da situação, mas é uma boa prática).No começo, cheirava a gambiarra, mas depois vi que performou muito bem, tendo um aumento de cerca de apenas 10% no tempo de processamento total (não apenas do banco de dados, considerando latência etc.). Logo percebi que era justamente o modo não relacional de fazer esse tipo de consulta.Claro que uma modelagem dos dados pensada para essa finalidade desde a concepção poderia considerar a incorporação dos campos de interesse em cada documento, o que evitaria a necessidade de junções. Mas não foi esse o caso.Uma ressalva importante aqui é que essa consulta funcionou muito bem para o problema que tinha, que envolvia num estágio anterior um belo filtro que fazia com que a quantidade de documentos a essa altura do pipeline fosse já drasticamente reduzida. A performance desse tipo de consulta vai variar para cada aplicação e também deve considerar a existência de índices. Se você for buscar algo diferente da chave primária ou fazer consultas com correspondências compostas, considere ajustar índices para garantir uma boa performance.É isso." }, { "title": "CI com Github Actions em projeto python com Django e PostgreSQL", "url": "/posts/CI-com-Github-Actions-em-projeto-python-com-Django-e-PostgreSQL/", "categories": "DevOps, Github Actions", "tags": "github actions, devops, ci, integração contínua, python, django, postresql", "date": "2022-03-07 00:00:00 -0300", "snippet": "Tenho dedicado um tempo a me familiarizar com a forma de construir pipelines de integração contínua com o github actions e, depois de algumas tentativas de construir uma para um projeto Django + PostgreSQL, resolvi documentar.Um primeiro arquivo do CIO github não quer ver a gente fazendo esforço e trata logo de sugerir alguns modelos de CI muito úteis com base nos arquivos do projeto. Por exemplo, abaixo alguns modelos e o que escolhi pra configurar um primeiro workflow considerando apenas o django, com testes e linting.Em relação ao modelo sugerido alterei apenas a última linha para adequar à estrutura de pastas do projeto e o array em python-version. O fluxo todo usa apenas a máquina principal, sem container de serviço, e ficou assim:name: Django CIon: push: branches: [ main ] pull_request: branches: [ main ]jobs: build: runs-on: ubuntu-latest strategy: max-parallel: 4 matrix: python-version: [3.7] steps: - uses: actions/checkout@v2 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v2 with: python-version: ${{ matrix.python-version }} - name: Install Dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Run Tests run: | python app/manage.py test app/ &amp;&amp; flake8 --config app/.flake8Uma breve explicação sobre cada parte que nem de longe substitui uma necessária lida na documentação que eu aconselho fortemente: Na seção on definimos os eventos que disparam o fluxo de trabalho, neste caso, em caso de push e pull request na branch main. Na seção strategy podemos definir uma matriz para diferentes configurações dos jobs. Aqui podemos por exemplo definir diferentes sistemas operacionais, arquiteturas e versões do python para usar no setup, mas optei por usar apenas uma (porquê? senti que deveria poupar recursos do github rs, afinal nesse primeiro momento queria primeiro fazer funcionar). Definindo dois sistemas operacionais diferentes e 3 versões do python, por exemplo, teríamos uma matriz com 6 trabalhos. Como estou usando apenas uma versão do python, na prática estou criando uma matriz de apenas 1 trabalho e não precisaria usar o strategy, mas preferi deixar a estrutura pronta para caso sinta a necessidade. As duas primeiras etapas do build são basicamente ações prontas (actions/checkout@v2 e actions/setup-python@v2) que usamos no nosso job, assim apenas referenciamos e não precisamos nos preocupar com algumas etapas usuais do processo.Abaixo o resultado do job, executando etapa de teste unitário e linting com flake8.Depois de atualizar o banco de dados do projeto para o PostgreSQL, precisei pesquisar como fazer isso no github actions.Adicionando um container de serviçoCom o SQLite, apenas a própria máquina principal com o projeto django é suficiente para rodar os testes. Adicionando um banco como PostgreSQL ou MySQL, precisamos criar um container de serviço, semelhante ao que é feito com o docker-compose.Variáveis de ambienteVamos usa um container e a máquina runner (a do projeto django) precisa saber como acessar o banco de dados no container; vamos fazer isso usando variáveis de ambiente. É possível definir variáveis de ambiente no contexto env que serão utilizados nos jobs em três níveis do fluxo (com base na própria documentação): Todo fluxo de trabalho, usando env no nível superior do arquivo. O conteúdo de um job em um fluxo de trabalho, usando jobs.&lt;job_id&gt;.env; Uma etapa específica dentro de um job, usando jobs.&lt;job_id&gt;.steps[*].env.Isso é muito importante e ajuda especialmente nessas integrações. Então, defini variáveis de configuração do banco de dados no segundo nível, no escopo do único job do fluxo - elas portanto ficam disponíveis em todo o job, incluindo nas etapas finais em que são executadas as migrações e os testes. Como o nome das variáveis de ambiente utilizadas pela imagem do postgres são diferentes, referenciei com ${{ env.&lt;VAR&gt; }}. Isso já resolve um eventual problema de credenciais inválidas.name: Django CIon: push: branches: [ main ] pull_request: branches: [ main ]jobs: build: runs-on: ubuntu-latest env: DB_HOST: localhost DB_NAME: postgres DB_USER: postgres DB_PASSWORD: supersecretpassword services: postgres: image: postgres:10-alpine env: POSTGRES_DB: ${{ env.DB_NAME }} POSTGRES_USER: ${{ env.DB_USER }} POSTGRES_PASSWORD: ${{ env.DB_PASSWORD }} # Set health checks to wait until postgres has started options: &gt;- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 ports: - 5432:5432 strategy: max-parallel: 4 matrix: python-version: [3.7] steps: - uses: actions/checkout@v2 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v2 with: python-version: ${{ matrix.python-version }} - name: Install postgres prerequisites run: sudo apt-get install libpq-dev - name: Install Dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Run migrations run: python app/manage.py wait_for_db &amp;&amp; python app/manage.py migrate - name: Run Tests run: | flake8 --config app/.flake8 &amp;&amp; python app/manage.py test app/Há muito mais sobre variáveis de ambiente no github actions que você pode querer saber e a documentação é excelente.Abaixo uma explicação mais pormenorizada sobre o container de serviço e as modificações na máquina runner.Container de serviçoCitando a documentação: Os containers de serviço são containers do Docker que fornecem uma maneira simples e portátil de hospedar serviços que você pode precisar para testar ou operar seu aplicativo em um fluxo de trabalho. Por exemplo, seu fluxo de trabalho pode precisar executar testes de integração que exijam acesso a um banco de dados e cache de memória.No meu caso, tinha uma máquina runner com o projeto django e precisava de apenas um container de serviço para o PostgreSQL. A documentação é vasta e inclui um excelente guia para nos ajudar no processo. Minha imagem ficou configurada assim:# jobs.build services: postgres: image: postgres:10-alpine env: POSTGRES_DB: ${{ env.DB_NAME }} POSTGRES_USER: ${{ env.DB_USER }} POSTGRES_PASSWORD: ${{ env.DB_PASSWORD }} # Set health checks to wait until postgres has started options: &gt;- --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5 ports: - 5432:5432As variáveis de ambiente definidas para o job foram acessadas e seus valores foram usados para definição das variáveis de ambiente esperadas pela imagem postgres:10-alpine, conforme já explicado. Em options podemos usar um recurso para aguardar até o postgres estar pronto. Precisamos também mapear a porta do container de serviço para o host porque, por padrão, o container de serviço não expõe a porta - nada diferente do esperado.Requisitos para a máquina runnerUm último e importante ajuste é o de garantir que a máquina runner tenha o que é preciso para usar o PostgreSQL. Precisei instalar as dependências necessárias para o pacote psycopg2. Além disso, no CI anterior não tinha a etapa que executava as migrações, então adicionei. Tem-se então as etapas Install postgres prerequisites e Run migrations.# em jobs.build steps: - uses: actions/checkout@v2 - name: Set up Python ${{ matrix.python-version }} uses: actions/setup-python@v2 with: python-version: ${{ matrix.python-version }} - name: Install postgres prerequisites run: sudo apt-get install libpq-dev - name: Install Dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Run migrations run: python app/manage.py wait_for_db &amp;&amp; python app/manage.py migrate - name: Run Tests run: | flake8 --config app/.flake8 &amp;&amp; python app/manage.py test app/O resultado (após, claro, um frutífero processo de falha → mais uma lida na documentação → ajuste → nova tentativa).É isso." }, { "title": "SQL espacial e otimização: ligando pontos a linhas pela proximidade", "url": "/posts/SQL-espacial-e-otimizacao-ligando-pontos-a-linhas-pela-proximidade/", "categories": "Banco de dados, PostGIS", "tags": "banco de dados, postgresql, postgis, geotecnologias", "date": "2022-02-14 00:00:00 -0300", "snippet": "Um tempo atrás precisei encontrar uma forma de relacionar um grande volume de geometrias pela localização no PostGIS. Tinha uma base com centenas de milhares de pares de coordenadas de GPS de veículos e uma malha de logradouros extraídas do OpenStreetMap em que cada feição era um segmento da via, e precisava responder a seguinte pergunta:Dado um par de coordenadas registrado pelo GPS, em que rua ele está?Precisava associar cada ponto a uma linha pela proximidadePreferencialmente, essa consulta deveria consumir a menor quantidade de recursos e executar no menor tempo possível. Solucionar esse problema me fez assimilar alguns conceitos muito importantes de consultas espaciais no PostGIS e acabei revisitando essa semana enquanto revia algumas anotações de estudo.Bom, vamos lá.Primeiro, precisei gerar uma geometria do tipo ponto a partir das coordenadas GPS, que estavam no formato geográfico decimal:ST_Transform(ST_SetSRID(ST_MakePoint(long, lat), 4326), 31985)Basicamente, criamos um ponto com ST_MakePoint, depois definimos o sistema de referência espacial usando o código EPSG com ST_SetSRID e, por fim, transformamos para o sistema métrico, no qual a base de logradouros também estava.Ambas as bases tinham índices espaciais.Ok. Vamos à primeira consulta que pensei:SELECT g.id gid, o.id oid, o.name logradouro FROM gps g, jp_osm o WHERE ST_Contains(ST_Buffer(o.geom, 10), g.geom)Essa consulta apresenta alguns problemas: Demorou muito para executar, não é performática; ST_Buffer não é uma boa ideia para seleção espacial por proximidade pois não gera um buffer exato, apenas a aproximação de um; A consulta acaba associando cada ponto a mais de um logradouro.Usar ST_Distance para filtrar também não seria uma boa ideia porque ela não pode se aproveitar dos índices espaciais. Ela faria uma varredura para encontrar todas as combinações de distância e isso seria muito, muito lento.Então, depois de alguns testes e leitura de documentação, cheguei a essa consulta:SELECT DISTINCT ON (g.id) g.id gid, o.id oid, o.name logradouro FROM gps g LEFT JOIN jp_osm o ON ST_DWithin(o.geom, g.geom, 10) WHERE o.id IS NOT NULL ORDER BY g.id, ST_Distance(g.geom, o.geom)Ela encontra a linha mais próxima de cada ponto dentro de uma tolerância de 10 metros, ou seja, acha o segmento de via mais provável em que cada veículo estava em cada registro do posicionamento terrestre. Essa consulta se mostrou superior como solução do problema que tinha pelos seguintes motivos: Executa em menos de 3% do tempo da outra consulta, ou seja, muito mais performática; Elimina duplas associações. Com a cláusula DISTINCT ON, que retorna apenas a primeira linha de um grupo distinto definido como parâmetro, é selecionada apenas a primeira linha com id de GPS distinto – DISTINCT ON (g.id) – e, ao ordenar pela menor distância entre o ponto e cada logradouro – ST_Distance(g.geom, o.geom) -, isso significa que apenas a associação com a linha mais próxima é selecionada; Usa ST_DWithin, que pode usar índices e é mais precisa para a seleção de geometrias dentro de uma certa distância do que a combinação ST_Contains + ST_Buffer;Resolver esse problema acabou sendo muito útil não só pela finalidade, também pelo fato de que aprendi muitos conceitos importantes para consultas espaciais.Espero que seja útil pra mais alguém que veja isso e precise solucionar um problema parecido." }, { "title": "Como gerar mapa de fluxos automaticamente no QGIS", "url": "/posts/Como-gerar-mapa-de-fluxos-automaticamente-no-QGIS/", "categories": "SIG, QGIS", "tags": "sig, qgis, camada virtual, sql, geotecnologias", "date": "2020-11-15 00:00:00 -0300", "snippet": "Os mapas de fluxo, em sistemas de informação geográfica, constituem um recurso muito utilizado para analisar os deslocamentos de pessoas e cargas entre áreas ou pontos de um território. No QGIS3, é possível gerar esses mapas dinamicamente através de uma camada virtual. Com algumas linhas de consulta SQL, configuramos uma camada que vetoriza automaticamente as linhas que representarão os fluxos e que ainda se atualizará dinamicamente se os dados de fluxo ou as geometrias de referência forem alteradas. Vejamos como funciona.No QGIS3 é possível criar mapas de fluxo com camada virtualNeste exemplo, eu vou utilizar os dados da Matriz de Origem/Destino real de deslocamentos de pessoas elaborada a partir de Big Data da telefonia móvel, apresentada e mantida pela Secretaria Nacional de Aviação Civil do Ministério da Infraestrutura, disponível aqui.Vou utilizar três camadas: aeroportos: a camada de aeroportos, que contém um campo chamado ICAO com o código de cada aeroporto; fluxo: uma tabela com os dados dos fluxos com embarque no Aeroporto Presidente Castro Pinto (código SBJP), na Paraíba, no mês de janeiro, que também contém campos com os códigos dos aeroportos de embarque e desembarque. Utilizaremos esses campos para fazer a relação; Uma camada dos estados brasileiros, como base, para melhor ilustração.Aqui:A consulta:SELECT f.aerodromoembarque, f.aerodromodesembarque, f.fluxo, make_line(a1.geometry, a2.geometry) FROM fluxo f JOIN aeroportos a1 ON f.aerodromoembarque = a1.ICAO JOIN aeroportos a2 ON f.aerodromodesembarque = a2.ICAOConsulta para criar linhas para o mapa de fluxosEstamos fazendo duas uniões importantes com a mesma camada, a de aeroportos, duplicando-a (a1 e a2): em uma, fazemos a união pelo código do aeroporto de embarque (a1); na outra, pelo de desembarque (a2). A função make_line faz o trabalho de desenhar uma linha ligando cada par de origem/destino.O resultado:Agora é só estilizar conforme sua necessidade. Por exemplo, buscando melhor representação, abaixo classifiquei por espessura da linha em 7 classes, utilizando quebras naturais, e apliquei transparência.É isso. Rápido, prático e escalável.Espero que ajude. Até a próxima." }, { "title": "Modelagem e mapeamento automáticos de um shapefile no GeoDjango", "url": "/posts/Modelagem-e-mapeamento-automaticos-de-um-shapefile-no-GeoDjango/", "categories": "Python, Django", "tags": "python, django, geodjango, geotecnologias", "date": "2020-07-04 00:00:00 -0300", "snippet": "Um recurso muito útil e que pode economizar tempo e trabalho quando você está fazendo a modelagem de dados geográficos no GeoDjango é automatizar a escrita do modelo e do dicionário de mapeamento.Já fiz essa escrita de forma manual algumas vezes, mas hoje digo seguramente que, independentemente da quantidade de campos, é muito melhor automatizar o processo, fazendo apenas um ou outro ajuste antes de executar a migração e a importação do shape.Você pode fazer isso com o ogrinspect. A sintaxe é essa:python manage.py ogrinspect [options] &lt;data_source&gt; &lt;model_name&gt; [options]No exemplo abaixo, vamos gerar de forma automática o modelo e o meapeamento de um shape de polígonos. São os bairros da cidade de João Pessoa. O shape está disponível aqui. Você precisa executar o comando na pasta raiz do seu projeto. Para manter as coisas organizadas, crie uma pasta data dentro da pasta do seu aplicativo e coloque os arquivos lá. Neste exemplo, a pasta do aplicativo é geo. No terminal, execute:python manage.py ogrinspect geo\\data\\Bairros.shp Bairro --srid=4326 --mappingA opção –srid define o sistema de referência espacial. A opção –mapping informa que queremos também o dicionário de mapeamento, para usarmos com o LayerMapping. O retorno deve ser algo parecido com isso:from django.contrib.gis.db import modelsclass Bairro(models.Model): bairro = models.CharField(max_length=100) cód = models.IntegerField() cod_loc = models.FloatField() bairro_1 = models.CharField(max_length=254) densidade = models.FloatField() perim_km = models.FloatField() area_km2 = models.FloatField() geom = models.PolygonField(srid=4326)# Auto-generated `LayerMapping` dictionary for Bairro modelbairro_mapping = { 'bairro': 'BAIRRO', 'cód': 'CÓD', 'cod_loc': 'Cod_loc', 'bairro_1': 'Bairro_1', 'densidade': 'Densidade', 'perim_km': 'Perim_km', 'area_km2': 'Area_km2', 'geom': 'POLYGON',}É isso. Espero que seja útil.Até mais." } ]
